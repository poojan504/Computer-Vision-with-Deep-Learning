# -*- coding: utf-8 -*-
"""RESNET

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PoTZJl68IQW2kffFn4mFsWcG20aKaK-T
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import tensorflow_hub as hub
import math,os,datetime

BATCH_SIZE = 32

train_data,train_info = tfds.load('cifar10',split='train[10%:90%]',with_info=True)
val_data = tfds.load('cifar10',split='train[0%:10%]')
test_data = tfds.load('cifar10',split='test')
print(train_data)

num_train_data = 0
for _ in train_data:
  num_train_data +=1
print(num_train_data)

num_val_data = 0
for _  in val_data:
  num_val_data+=1
print(num_val_data)

train_steps_per_epoch = math.ceil(num_train_data/BATCH_SIZE)
val_steps_per_epoch = math.ceil(num_val_data/BATCH_SIZE)

def normalizer(features,input_shape = [224,224,3],augment=True,seed=42):
  input_shape = tf.convert_to_tensor(input_shape)
  image = features['image']
  image = tf.image.convert_image_dtype(image,tf.float32)
  if augment:
    ## Randomly applied horizontal flip:
        image = tf.image.random_flip_left_right(image, seed=seed)

        # Random B/S changes:
        image = tf.image.random_brightness(image, max_delta=0.1, seed=seed)
        image = tf.image.random_saturation(image, lower=0.5, upper=1.5, seed=seed)
        image = tf.clip_by_value(image, 0.0, 1.0) # keeping pixel values in check

        # Random resize and random crop back to expected size:
        
        random_scale_factor = tf.random.uniform([1], minval=1., maxval=1.4, dtype=tf.float32, seed=seed)
        scaled_height = tf.cast(tf.cast(input_shape[0], tf.float32) * random_scale_factor, 
                                tf.int32)
        scaled_width = tf.cast(tf.cast(input_shape[1], tf.float32) * random_scale_factor, 
                               tf.int32)
        scaled_shape = tf.squeeze(tf.stack([scaled_height, scaled_width]))
        image = tf.image.resize(image, scaled_shape)
        image = tf.image.random_crop(image, input_shape, seed=seed)
  else:     
    image = tf.image.resize(image,input_shape[:2])
  label = features['label']
  features = (image,label)
  return features

train_data = train_data.map(normalizer)
val_data = val_data.map(normalizer)

print(train_data)
print(val_data)

class_names = train_info.features["label"].names
print(class_names)

plt.figure(figsize=(10,10))
for i , (image,label) in enumerate(train_data.take(24)):
  #image = image.numpy().reshape([28,28,3])
  plt.subplot(5,5,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.grid(False)
  plt.imshow(image,cmap=plt.cm.binary)
  plt.xlabel(class_names[label])
plt.show()

train_data = train_data.batch(BATCH_SIZE)
val_data = val_data.batch(BATCH_SIZE)
train_data = train_data.prefetch(1)
val_data = val_data.prefetch(1)
#train_data = train_data.cache()

print(val_data)
print(train_data)

Resnet = tf.keras.applications.ResNet50()

Resnet.summary()

model_dir = './models/Resnet'
logdir = os.path.join("logs", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
callbacks = [
    # Callback to interrupt the training if the validation loss/metrics stops improving for some epochs:
    tf.keras.callbacks.EarlyStopping(patience=8, monitor='val_acc',
                                    restore_best_weights=True),
    # Callback to log the graph, losses and metrics into TensorBoard:
  
        tf.keras.callbacks.TensorBoard(model_dir, histogram_freq=1,write_graph=True)

  # Callback to simply log metrics at the end of each epoch (saving space compared to verbose=1/2):
]
Resnet.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=
              [tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),
               tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5,name='top5_acc')])

history = Resnet.fit(train_data,epochs=30,steps_per_epoch=train_steps_per_epoch,validation_data=val_data,validation_steps=val_steps_per_epoch,verbose=1)

LeNet.save_weights("lenet_weights",overwrite=True)
LeNet.save('My_ResNet")

from google.colab import drive
drive.mount('/content/drive')

cd drive/MyDrive/LeNet/content/

ls

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

rm -rf ./logs/

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir models/Resnet

