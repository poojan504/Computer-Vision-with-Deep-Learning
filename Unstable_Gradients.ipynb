{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMOJunvr8gMQ8+a/8a3cbIS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poojan504/Computer-Vision-with-Deep-Learning/blob/master/Unstable_Gradients.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The unstable graidents issue:-\n",
        "1. vanishing gradients\n",
        "    in case of vanishing gradients, the relation between hidden layers and the output layers tends to flatten with the increase in layers from output layers.\n",
        "2. exploding gradients\n",
        "    in case of exploding gradients, the relation between hideen layers and the output layers tends to steep with increase in layers from output layer resulting in the saturating the ability to learn for the whole nn.\n",
        "\n",
        "The solution to the unstable gradient is batch normaliztion. \n",
        "it allows layer to learn more independetly.\n",
        "it allows for the selection of higher learning rate.\n",
        "layer output is normalized with batch meand stddev thus it adds a noise element\n",
        "which helps regulizing the data which gives a generalization between the training data and the data that has not been encountered from the model\n",
        "\n"
      ],
      "metadata": {
        "id": "VNWgRPtbf0pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "model regularizarion:-\n",
        "\n",
        "in the case of training cost goes down but the validation cost goes up is formally none as overfitting.\n",
        "\n",
        "mostly used regularization techniques:-\n",
        "1. L1 regularization\n",
        "2. L1/L2 regularization\n",
        "3. dropout\n",
        "4. data augmentation.\n",
        "\n",
        "adding drop out starting from the last layer is a good practice. (20% to 50%) for vision.\n"
      ],
      "metadata": {
        "id": "S2s5PBwzif-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OYsjkTuKmMm9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irafWF5qfwxI"
      },
      "outputs": [],
      "source": []
    }
  ]
}