# -*- coding: utf-8 -*-
"""LeNet_Prediction_without_augmentation.ipyb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oC9mpOmTTW9VFHTIxzTO3JpjhYKZMDgW
"""

!unzip "/content/drive/My Drive/LeNet/LeNet.zip" -d "/content/drive/My Drive/LeNet"

cd drive/MyDrive/LeNet/content/

import tensorflow as tf
import tensorflow_datasets as tfds
import cifar_utils
import matplotlib.pyplot as plt
from  tensorflow.keras import datasets,layers,models,losses
from sklearn.model_selection import train_test_split
import numpy as np
import matplotlib.pyplot as plt
import tensorflow_hub as hub
import collections
import functools
import math
import datetime,os
import functools

BATCH_SIZE = 32
EPOCHS =300
Inception_input_shape = [299,299,3]
random_seed = 42

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

cifar_info = cifar_utils.get_info()
print(cifar_info)
#train_data , test_data = datasets['train'],datasets['test']

cifar_info.features['label'].names

test_data,test_info = tfds.load('cifar10',split='test',with_info=True)
val_data ,val_info= tfds.load('cifar10',split='train[:10%]',with_info=True)
train_data ,train_info= tfds.load('cifar10',split='train[10%:100%]',with_info=True)

num_train_examples = 0
for _ in train_data:
  num_train_examples+=1
print(num_train_examples)

num_val_examples=0
for _ in val_data:
  num_val_examples +=1
print(num_val_examples)

tfds.as_dataframe(train_data.take(10),train_info)

tfds.as_dataframe(val_data.take(10),val_info)

def normalizer(features,input_shape = [299,299,3]):
  input_shape = tf.convert_to_tensor(input_shape)
  image = features['image']
  image = tf.image.convert_image_dtype(image,tf.float32)
  image = tf.image.resize(image,input_shape[:2])
  features['image'] = image
  return image

train_data = train_data.map(normalizer)

val_data = val_data.map(normalizer)

train_data = train_data.batch(BATCH_SIZE)
val_data = val_data.batch(BATCH_SIZE)
train_data = train_data.prefetch(1)
val_data = val_data.prefetch(1)
print(val_data)

LeNet = tf.keras.models.load_model("My_LeNet")

LeNet.summary()

num_train_imgs = train_info.splits['train'].num_examples
num_val_imgs = val_info.splits['test'].num_examples
train_steps_per_epoch = math.ceil(num_train_imgs/BATCH_SIZE)
val_steps_per_epoch = math.ceil(num_val_imgs/BATCH_SIZE)

import glob
import numpy as np
from classification_utils import load_image, process_predictions, display_predictions
inception_expected_input_shape = [299, 299, 3]
test_filenames = glob.glob(os.path.join('/content/drive/My Drive/LeNet/res', '*'))
test_images = np.asarray([load_image(file, size=inception_expected_input_shape[:2]) 
                          for file in test_filenames])
print('Test Images: {}'.format(test_images.shape))

image_batch = test_images[:16]

# Our model was trained on CIFAR images, which originally are 32x32px. We scaled them up
# to 224x224px to train our model on, but this means the resulting images had important
# artifacts/low quality.
# To test on images of the same quality, we first resize them to 32x32px, then to the 
#expected input size (i.e., 224x224px):
cifar_original_image_size = cifar_info.features['image'].shape[:2]
class_readable_labels = cifar_info.features["label"].names

image_batch_low_quality = tf.image.resize(image_batch, cifar_original_image_size)
image_batch_low_quality = tf.image.resize(image_batch_low_quality, inception_expected_input_shape[:2])
    
predictions = LeNet.predict_on_batch(image_batch_low_quality)
top5_labels, top5_probabilities = process_predictions(predictions, class_readable_labels)

print("Inception Predictions:")
display_predictions(image_batch, top5_labels, top5_probabilities)